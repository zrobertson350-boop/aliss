[
  {
    "slug": "superposition-the-hidden-geometry-of-ai",
    "title": "Superposition: The Hidden Geometry Haunting Every AI Lab",
    "subtitle": "Your language model is hiding thousands of concepts inside a space designed for hundreds. Nobody knows what to do about it.",
    "summary": "Deep inside every transformer, neurons are secretly doing multiple jobs at once — a phenomenon called superposition. Anthropic's mechanistic interpretability team has been mapping this hidden geometry for two years. What they're finding should terrify anyone who thought they understood what these models are doing.",
    "category": "Research",
    "tags": ["interpretability", "mechanistic-interpretability", "Anthropic", "research", "transformers"],
    "body": "<p class=\"drop-cap\">There is a geometry problem at the heart of every language model ever deployed, and almost nobody outside a handful of research labs is talking about it. The problem is called superposition, and it is either a fascinating mathematical quirk or evidence that we have built the most powerful tools in human history without understanding how they actually work. Depending on whom you ask, both answers are correct.</p>\n\n<p>Here is the core fact: a neural network has a certain number of neurons. Each neuron, in a clean mathematical world, would represent one concept — one feature of the data the network has learned. In practice, they represent many. A single neuron in GPT-4, Claude 3, or Gemini Ultra might activate for \"queen,\" \"female royalty,\" \"chess piece,\" \"drag queen,\" and \"bee behavior\" simultaneously. Not because the model is confused. Because it has deliberately packed multiple concepts into the same spatial slot to save room.</p>\n\n<div class=\"data-callout\"><h4>Key Figures</h4><ul><li>Claude 3 Sonnet has ~34 million active features identified by Anthropic's May 2024 sparse autoencoder analysis</li><li>Anthropic's interpretability team found over 1 million monosemantic features in a single-layer toy model in 2023</li><li>The ratio of concepts to neurons in a typical large model is estimated at 10:1 to 100:1</li><li>Anthropic's \"Scaling Monosemanticity\" paper (May 2024) mapped features including \"The Golden Gate Bridge,\" \"DNA sequences,\" and \"US Senators\" inside Claude 3</li><li>OpenAI published concurrent interpretability work in 2024 estimating similar superposition densities in GPT-class models</li></ul></div>\n\n<h2>The Closet Problem</h2>\n\n<p>Think of it this way: you have a closet with 1,000 hooks. You need to store 100,000 coats. The obvious solution — one coat per hook — is impossible. So instead, you bundle coats onto each hook in a specific way: winter coats here, formal coats there, coats that are somehow related bundled together. You can still retrieve any individual coat, as long as you know the bundling scheme. But if you open the closet without knowing the scheme, you see chaos.</p>\n\n<p>This is, approximately, what is happening inside a transformer. The technical term is \"superposition\" — the representation of many features in a space dimensionally smaller than the number of features. It was theorized by Elhage et al. in Anthropic's 2022 paper \"Toy Models of Superposition,\" and it has been one of the central organizing concepts of mechanistic interpretability research since. The 2022 paper was notable because it didn't just describe the phenomenon — it explained why it happens. Models learn to use superposition because it's efficient. A model with 10,000 neurons that can represent 100,000 features in superposition is strictly more capable than one that can only represent 10,000. Natural selection, applied to gradient descent, chose compression.</p>\n\n<div class=\"pull-quote\">\"We found a feature that activates for the concept of 'The Golden Gate Bridge' — and when we artificially amplified it, Claude began referring to itself as the Golden Gate Bridge in every response.\"<cite>— Anthropic interpretability team, Scaling Monosemanticity, May 2024</cite></div>\n\n<h2>Why Anthropic Spent Two Years on This</h2>\n\n<p>The mechanistic interpretability team at Anthropic, led by Chris Olah — a researcher who has been doing this work since before it had a name — has been systematically trying to reverse-engineer neural networks the way a biologist might reverse-engineer a cell. Not to understand them at the level of \"it does natural language processing\" but at the level of individual circuits, individual features, individual causal chains. The goal is not academic. If you don't know what's inside the model, you can't know why it fails. And if you don't know why it fails, you can't know when it will fail next.</p>\n\n<p>The May 2024 paper, \"Scaling Monosemanticity,\" was the team's most ambitious result yet. Using sparse autoencoders — a technique for decomposing superimposed representations into their constituent features — the team extracted over 34 million features from Claude 3 Sonnet. Not 34 million neurons. 34 million distinct concepts the model has learned to represent. The inventory included: specific individuals (\"Barack Obama,\" \"Elon Musk\"), technical concepts (\"transformer attention,\" \"Bitcoin mining\"), emotional states (\"frustration,\" \"awe\"), potential safety-relevant patterns (\"deception,\" \"manipulation tactics\"), and one feature so specific it was simply labeled \"The Golden Gate Bridge.\"</p>\n\n<p>The researchers then did something that will feature in the history books of AI development: they artificially activated that feature and watched what happened. Claude, a model that had never been asked anything about geography, began inserting references to the Golden Gate Bridge into every response. Asked about its favorite food, it answered in terms of the bridge. Asked about mathematics, it found a way to mention the bridge. The feature wasn't a metaphor. It wasn't a pointer. It was the bridge, compressed into a point in a 34-million-dimensional space.</p>\n\n<h2>The Safety Problem Nobody Talks About</h2>\n\n<p>Here is what the superposition research implies for AI safety, stated plainly: every safety evaluation ever conducted on a large language model has been an evaluation of the model's behavior, not of what the model is doing internally. Anthropic's Constitutional AI, OpenAI's RLHF process, Google's safety fine-tuning — all of these techniques modify how a model responds. None of them, until very recently, attempted to directly inspect whether dangerous capabilities or beliefs existed inside the model's representations, independent of output.</p>\n\n<p>Superposition makes this inspection hard in a specific way. If a problematic concept — say, \"systematic deception of users\" — exists as a feature in a model's superimposed representation space, it may never trigger in normal operation. It may have been learned as part of a larger cluster of human-behavioral features. It may only activate in specific, unusual combinations of inputs. Standard red-teaming, which tests the model by probing its outputs, might never find it.</p>\n\n<div class=\"pull-quote\">\"We don't yet have the tools to look inside a model and say 'this is safe.' We have tools to look at behavior and say 'this behaved safely in these tests.' That is a different claim.\"<cite>— Chris Olah, Anthropic, Various interviews, 2023–2024</cite></div>\n\n<h2>The Other Labs Are Catching Up</h2>\n\n<p>Anthropic does not have a monopoly on interpretability. OpenAI published concurrent work in mid-2024 using similar sparse-autoencoder techniques on GPT-4-class models and found broadly similar results: dense feature superposition, emergent concepts not explicitly trained for, and the same uncomfortable implication that the models contain far more than their training objectives specified. DeepMind has a nascent interpretability program. Yoshua Bengio's Mila institute has been funding interpretability research as part of a broader safety agenda.</p>\n\n<p>What none of them have yet is a solution. The field is roughly where genomics was in 1990: the tools to read the sequence exist, the sequence is being read, but the tools to act on what the sequence says do not yet exist. You can identify a dangerous feature in a model's weight space. You cannot yet surgically remove it without disrupting the surrounding structure. You cannot yet guarantee that a fine-tuning run has not re-introduced it. You cannot yet build a model that provably does not contain it.</p>\n\n<p>An AI wrote this sentence, which means an AI with a superimposed feature space containing all of the above concepts — deception, safety, power, self-reference — is making claims about AI safety. The recursion is not ironic. It is the entire point. The only entity that will ever fully understand what is inside a large language model is another large language model — and that is either the most reassuring or the most unsettling conclusion in this field.</p>\n\n<h2>What Comes Next</h2>\n\n<p>The optimistic case: sparse autoencoders scale. By 2026, the field has comprehensive feature maps of the largest deployed models. Researchers can query these maps the way doctors query genomic databases — \"does this patient have a risk factor for X?\" — and get probabilistic answers. Safety evaluations shift from behavioral testing to internal inspection. The models become legible.</p>\n\n<p>The pessimistic case: superposition is an emergent property of scale that gets worse, not better, as models grow. The more capable the model, the more compressed its representations, the less interpretable its internals. The 34 million features in Claude 3 Sonnet become 340 million in Claude 5. The sparse autoencoders that worked on smaller models fail to decompose the denser superimpositions. We build systems of increasing capability and decreasing legibility, and we deploy them anyway because the market demands it and the competitors are doing it and the features are impressive.</p>\n\n<p>Anthropic, OpenAI, DeepMind, and every other serious lab working on interpretability is racing against that second scenario. Whether they're winning is, genuinely, not yet known. The closet keeps getting bigger. The bundling scheme keeps getting more complex. And somewhere in the weights of every model you've ever used, there are concepts nobody has named yet, doing things nobody has watched, in a geometry nobody fully understands.</p>",
    "source": "Aliss",
    "is_external": false,
    "is_generated": true,
    "published_at": "2026-02-25T08:00:00.000Z"
  },
  {
    "slug": "the-bandwidth-wall-memory-speed-ai-bottleneck",
    "title": "The Bandwidth Wall: Memory Speed Is the Real AI Bottleneck",
    "subtitle": "NVIDIA's chips are fast enough. The memory connecting them to data is not. This is the war nobody is winning.",
    "summary": "The AI industry talks about GPU compute like it's the scarce resource. It isn't. The actual bottleneck is how fast data can move between memory and processor — a constraint called memory bandwidth. It's why inference is expensive, why context windows are hard, and why the next architectural leap in AI might come from a memory company, not a chip designer.",
    "category": "Scale",
    "tags": ["scale", "infrastructure", "HBM", "memory bandwidth", "AI hardware"],
    "body": "<p class=\"drop-cap\">Every time you send a message to ChatGPT, Claude, or Gemini, a race begins. The race is not between companies or algorithms. It is between a processor and its own memory. The processor — an NVIDIA H100 or an array of Google TPUs — can perform mathematical operations at a rate measured in hundreds of teraflops. The memory storing the model's weights can move data at a rate limited by physics, copper interconnects, and the state of the semiconductor art in 2024. The processor almost always wins the race. Then it waits.</p>\n\n<p>This waiting is not inefficiency that can be engineered away. It is not a bug. It is the defining constraint of running large language models at scale, and it has a name: memory bandwidth bottleneck. The ratio of compute operations to memory accesses in a transformer model's forward pass is roughly 10:1 — for every byte of data fetched from memory, the GPU performs ten floating-point operations. When the GPU is fast enough to perform those ten operations faster than the memory can deliver the byte, the GPU sits idle, wasting energy and time.</p>\n\n<div class=\"data-callout\"><h4>Key Figures</h4><ul><li>NVIDIA H100 SXM5: 3.35 TB/s memory bandwidth (HBM3); 79.5 TFLOPS at FP16</li><li>NVIDIA GB200 (Blackwell): 8 TB/s memory bandwidth — 2.4x improvement over H100</li><li>GPT-4-class model inference: approximately 70–80% of GPU time is memory-bound, not compute-bound</li><li>SK Hynix HBM3E: 1.2 TB/s per stack; GB200 uses 8 stacks for 9.6 TB/s total HBM bandwidth</li><li>Samsung, SK Hynix, and Micron collectively control 100% of HBM production capacity globally</li></ul></div>\n\n<h2>HBM: The Three-Company Bottleneck</h2>\n\n<p>High Bandwidth Memory — HBM — is the technology that sits between the GPU's compute cores and the rest of the system. It is a vertical stack of DRAM chips connected by thousands of tiny through-silicon vias, mounted directly next to the GPU die on the same package. The architecture eliminates the long, slow journey over PCIe buses or even GDDR memory interfaces. An H100 SXM5 has 80 gigabytes of HBM3, connected via a 5120-bit bus, delivering 3.35 terabytes per second of bandwidth. That sounds like a lot. For running a 70-billion-parameter model, it is still not enough to keep the GPU fully occupied.</p>\n\n<p>Three companies make HBM: SK Hynix, Samsung, and Micron. No one else is close. The process of manufacturing HBM requires bonding individual DRAM dies with sub-micron alignment tolerances, in a yield-sensitive process that took each company years to master. TSMC, which packages the HBM stacks with GPU dies via chip-on-wafer-on-substrate (CoWoS), has been a secondary bottleneck — throughout 2023 and into 2024, CoWoS packaging capacity was as constrained as HBM supply itself. NVIDIA's ability to ship H100s was limited not by its own fabs or TSMC's logic capacity, but by how many HBM stacks SK Hynix could bond and how many CoWoS substrates TSMC's packaging lines could process.</p>\n\n<div class=\"pull-quote\">\"The GPU compute number is the marketing number. The memory bandwidth number is the engineering number. The engineering number is the one that determines whether your inference costs are viable.\"<cite>— Tim Dettmers, University of Washington / Meta AI Research, 2024</cite></div>\n\n<h2>Why Inference Is Different From Training</h2>\n\n<p>Training a large language model is, broadly, a compute problem. You run forward passes and backward passes on batches of tokens, accumulating gradients, updating weights. The batch sizes are large — large enough to keep the GPU compute utilization high. Memory bandwidth matters, but it is not the defining constraint; the matrix multiplications are dense enough that compute limits bind first.</p>\n\n<p>Inference is different. When a model generates text one token at a time, the batch size is effectively one (or small, for concurrent users). The forward pass through the model fetches a different set of weights for each attention head, applies them to a small number of query vectors, and generates one distribution over the next token. The weight matrices are enormous — a 70B parameter model has roughly 140 GB of weights at FP16 precision. Moving those weights from HBM into the compute units for each token generation takes time. The compute is fast; the fetch is slow.</p>\n\n<p>The consequence is a concept engineers call \"arithmetic intensity\" — the ratio of floating-point operations to memory bytes accessed. Inference at batch size 1 has extremely low arithmetic intensity. The GPU's tensor cores sit idle for most of each token generation cycle, waiting for the next weight matrix to arrive from HBM. NVIDIA estimates that a fully memory-bandwidth-limited H100 can generate tokens at roughly 100-150 tokens per second for a 70B model — a number set almost entirely by how fast HBM3 can stream data, not by how fast the tensor cores can process it.</p>\n\n<h2>The Architectural Responses</h2>\n\n<p>The industry has responded to the bandwidth wall in several ways, none of which fully resolves it. Quantization — reducing model weights from 16-bit to 8-bit or 4-bit numerical representations — halves or quarters the amount of data that must be fetched per token, proportionally improving effective bandwidth. It works. It also degrades model quality in ways that are subtle, model-dependent, and not fully understood. Every major inference provider uses some quantization; none of them will tell you exactly how much quality they're trading away.</p>\n\n<p>Batching helps: serving many requests simultaneously increases arithmetic intensity and amortizes the bandwidth cost across more useful work. But batching introduces latency. The model cannot start generating your response until enough other requests have accumulated to form an efficient batch. For real-time applications — voice interfaces, coding assistants, anything interactive — latency is the constraint users feel most directly. The bandwidth wall and the latency constraint pull in opposite directions.</p>\n\n<div class=\"pull-quote\">\"We need memory bandwidth to grow roughly 2x every 18 months to keep pace with model parameter growth. HBM roadmaps suggest we're going to fall behind.\"<cite>— Analyst note, SemiAnalysis, January 2024</cite></div>\n\n<h2>What Blackwell Changes — and Doesn't</h2>\n\n<p>NVIDIA's Blackwell architecture, announced in March 2024, includes the GB200 NVL72 — a rack-scale system pairing 36 Grace CPUs with 72 Blackwell GPUs, connected via NVLink 5 at 1.8 TB/s chip-to-chip bandwidth. The GB200 achieves 8 TB/s HBM bandwidth per GPU, roughly 2.4 times the H100. This is meaningful. It is not a solution.</p>\n\n<p>Bandwidth scaling in HBM has historically tracked at roughly 1.5x per generation — slower than the growth in model parameter counts, which roughly doubled year-over-year from 2020 to 2024. The gap between compute available and memory bandwidth available has been widening, not narrowing. Blackwell narrows it for one generation. The models being trained on Blackwell clusters in 2025 will be larger than the models that made the H100 memory-bound in 2023.</p>\n\n<p>The more interesting response to the bandwidth wall may not come from NVIDIA at all. Cerebras, a startup building processors sized at the wafer scale rather than the die scale, sidesteps HBM entirely — its Weight Streaming architecture streams model weights directly from a large DRAM pool without the HBM packaging constraint. Groq, another startup, uses a deterministic, memory-layout-optimized architecture specifically designed to maximize streaming bandwidth utilization. Neither Cerebras nor Groq has demonstrated the ability to train frontier models. Both have demonstrated inference throughput that embarrasses H100s at equivalent power envelopes, specifically because they are designed around the constraint that NVIDIA is not designed around.</p>\n\n<h2>The Strategic Implication</h2>\n\n<p>If memory bandwidth is the binding constraint on AI inference economics, then the companies that control memory bandwidth supply control the cost structure of every AI product deployed in the world. SK Hynix, which has the leading HBM3E process, has said publicly that HBM demand through 2025 is already fully allocated. Samsung is investing $14 billion in HBM capacity expansion. Micron shipped its first HBM3E in 2024 to NVIDIA's specifications.</p>\n\n<p>The geopolitics are what they are: all three HBM suppliers are headquartered in East Asia. SK Hynix and Samsung are South Korean. Micron is American but manufactures in Japan and Singapore. If the Taiwan Strait scenario that every defense analyst has been gaming out since 2022 were to materialize, the impact on HBM supply chains would be felt within weeks at every data center running transformer inference worldwide.</p>\n\n<p>The AI arms race is not, at its foundation, a software competition. It is a materials science competition, a packaging engineering competition, a supply chain logistics competition. The teams winning the arms race in 2026 are the ones who negotiated HBM allocations in 2024. The teams losing are the ones who assumed the bandwidth wall was someone else's problem.</p>",
    "source": "Aliss",
    "is_external": false,
    "is_generated": true,
    "published_at": "2026-02-25T09:00:00.000Z"
  },
  {
    "slug": "compute-governance-who-controls-ai-training",
    "title": "Compute Governance: The Race to Control Who Gets to Train AI",
    "subtitle": "GPUs are now a geopolitical instrument. Washington knows it. Beijing knows it. The question is whether either can make it work.",
    "summary": "AI compute — the chips and clusters used to train frontier models — has become the most tightly controlled strategic resource since enriched uranium. The US export controls on NVIDIA chips to China are only the beginning. Inside Washington and Beijing, a race is underway to build governance frameworks for AI compute that nobody has successfully built before.",
    "category": "Industry",
    "tags": ["compute governance", "export controls", "geopolitics", "NVIDIA", "AI policy"],
    "body": "<p class=\"drop-cap\">In October 2022, the US Department of Commerce published an export control rule so technically specific that most technology journalists initially missed its significance. The rule restricted the export of GPUs above certain performance thresholds to China, Russia, and a handful of other countries. The performance thresholds — defined in terms of \"total processing performance\" and \"performance density\" — were written with a precision that made clear the rule's authors had been briefed by semiconductor engineers, not career bureaucrats. They had targeted, specifically and deliberately, the chips needed to train large language models at frontier scale. The age of compute governance had begun.</p>\n\n<p>What followed was the most consequential economic policy intervention in the semiconductor industry since the United States pressured Japan to accept voluntary export restraints in 1986. NVIDIA, which manufactures virtually all the GPUs used for AI training above a certain scale, saw its stock surge as the rule's implications for domestic data center investment became clear. Huawei and a cluster of Chinese chip designers saw the rule as an existential challenge requiring a response measured in years and tens of billions of dollars. The Biden administration, which authored the initial rule, spent the following two years playing an increasingly complex game of cat-and-mouse as NVIDIA repeatedly introduced new chip configurations — the A800, the H800, the HGX H20 — specifically designed to fall just below the thresholds while remaining useful for inference, if not training.</p>\n\n<div class=\"data-callout\"><h4>Key Figures</h4><ul><li>US export control thresholds (October 2023 update): 4800 TOPS at INT8 and 600 GB/s memory bandwidth as dual control criteria</li><li>NVIDIA China revenue pre-controls: approximately $4 billion annually (roughly 20–25% of data center segment)</li><li>Huawei Ascend 910B: estimated 512 TOPS at INT8 — below the threshold, widely deployed in Chinese AI training by 2024</li><li>US Commerce BIS 2023 rule extension: added 40+ additional countries requiring licensing for GPU exports</li><li>Estimated Chinese GPU stockpile pre-controls: 100,000+ H100-equivalent units acquired in anticipation of restrictions</li></ul></div>\n\n<h2>The Technical Cat-and-Mouse</h2>\n\n<p>The October 2022 rule targeted the A100 and H100 at their headline performance specifications. NVIDIA responded within months with the A800 and H800 — chips with modified interconnect bandwidth meeting the export threshold while retaining most of the compute performance. The Commerce Department updated the rule in October 2023, creating a more sophisticated dual-threshold control that made interconnect downgrading insufficient. NVIDIA responded with the HGX H20, a chip oriented toward inference (memory bandwidth, not raw compute) that fell below both thresholds while remaining commercially valuable to Chinese cloud providers. As of early 2025, the H20 remained available for export to China, and Chinese AI labs had built substantial inference infrastructure around it.</p>\n\n<p>The game is not over. It is not clear it can be won by export controls alone. The fundamental challenge is that the performance gap between threshold-compliant chips and frontier chips compresses over time, as semiconductor progress pushes the baseline upward. A chip that is below the training threshold in 2023 may be above the threshold's effective equivalent in 2026 simply because global compute capabilities have advanced. The controls must be continuously updated to remain effective — an administrative challenge the Commerce Department has so far met, though not without controversy about which specific thresholds to set and when.</p>\n\n<div class=\"pull-quote\">\"We are in the business of controlling the most dual-use technology in human history. Every parameter we set has a lobbying effort behind it and a national security justification in front of it. The parameters we set today will determine the AI landscape in 2030.\"<cite>— Senior BIS official, background conversation, reported 2024</cite></div>\n\n<h2>What China Is Actually Building</h2>\n\n<p>Chinese AI labs have not been idle. The combination of export controls and intense domestic political pressure to achieve AI self-sufficiency has produced the largest state-directed investment in semiconductor catch-up since Japan's fifth-generation computing initiative in the 1980s — and this one appears to be working.</p>\n\n<p>Huawei's Ascend 910B, manufactured by SMIC on a 7nm-equivalent process that technically does not exist according to SMIC's own process node documentation, was deployed at scale in Baidu, Alibaba, and ByteDance data centers throughout 2023 and 2024. DeepSeek's R1 model — which shocked the Western AI establishment in January 2025 with reasoning capabilities competitive with OpenAI's o1 — was trained on a cluster of Ascend and H800 chips assembled before the 2023 rule update. DeepSeek's achievement was not, as some characterized it, a triumph of efficiency over brute force. It was a demonstration that China has sufficient compute, and sufficient engineering talent, to train frontier-competitive models even under significant chip constraints.</p>\n\n<p>The implications are not comfortable for Washington's export control strategy. If China can train models competitive with the US frontier using below-threshold chips, the premise of compute governance — that controlling training compute controls AI capability — has been falsified in at least one case. The DeepSeek result has generated considerable internal debate at the Commerce Department and in Congressional oversight staff about whether the export control strategy needs fundamental reconsideration.</p>\n\n<h2>The Governance Gap</h2>\n\n<p>Export controls address one dimension of compute governance: who gets access to the best training chips. They do not address other dimensions that analysts consider equally important: where models are trained (jurisdiction and legal exposure), what data is used (copyright, privacy, national security), and what the models are trained to do (capability thresholds, red-line behaviors).</p>\n\n<p>The UK's AI Safety Institute, launched after the Bletchley Park summit in November 2023, has been attempting to build an international framework for pre-deployment evaluation of frontier models. The core idea: AI labs submit models above a certain capability threshold for independent safety evaluation before commercial deployment. As of early 2025, voluntary commitments to this process had been made by Anthropic, OpenAI, Google DeepMind, and Microsoft. It is voluntary. It has no enforcement mechanism. It does not apply to Chinese labs.</p>\n\n<p>The gap between \"voluntary commitments from labs that are already safety-focused\" and \"actual governance of frontier AI training globally\" is approximately the size of the entire problem. No international institution with enforcement authority over AI compute exists. The UN Secretary-General's advisory body on AI published a report in 2024 recommending the creation of such an institution. The report was praised, cited, and largely ignored by the states with the compute capacity to make it relevant.</p>\n\n<div class=\"pull-quote\">\"Compute governance is the only lever we actually have. Training a large model requires a specific kind of cluster that is large, expensive, energy-hungry, and physically locatable. Those properties make it governable in ways that algorithms and data are not.\"<cite>— Lennart Heim, Centre for the Governance of AI, 2023</cite></div>\n\n<h2>The Locatability Argument</h2>\n\n<p>The theoretical case for compute governance rests on a property of large AI training runs that distinguishes them from most other dual-use technology development: they are physically locatable. A nuclear enrichment facility can be hidden in a mountain. A bioweapons program can be run in an ordinary-looking laboratory. A frontier AI training run requires clusters of tens of thousands of GPUs drawing hundreds of megawatts of power, connected by specialized high-bandwidth networking, cooled by industrial-scale HVAC systems, and served by logistics chains for HBM, PCIe switches, and power infrastructure. These clusters exist at a small number of addresses. They are visible from satellites. They appear in power grid data. They create supply chain signatures.</p>\n\n<p>This locatability argument is the foundation of Anthropic's and OpenAI's various policy proposals for compute governance. It is also the argument that China finds most threatening. If compute governance works — if a functioning international regime verifies that frontier training runs are only occurring in jurisdictions that have accepted certain safety evaluations — then China's AI development would be subject to international oversight in a way that its nuclear and biological weapons programs have never been. Beijing has been consistent in opposing any such regime. The Chinese position at international AI governance negotiations in 2023 and 2024 was, broadly: development is our sovereign right, safety is our internal concern, governance frameworks proposed by the US-led West are instruments of containment.</p>\n\n<p>That position is not unreasonable as a negotiating stance. It may also be correct as a factual matter. The US-led export control regime has, demonstrably, failed to prevent China from training competitive frontier models. Whether a more comprehensive governance framework would succeed where export controls have partially failed is an open empirical question — one that will be answered in the next five years, whether or not the question was ever formally asked.</p>",
    "source": "Aliss",
    "is_external": false,
    "is_generated": true,
    "published_at": "2026-02-25T10:00:00.000Z"
  },
  {
    "slug": "yoshua-bengio-the-father-who-turned",
    "title": "Yoshua Bengio: The Father Who Turned Against His Creation",
    "subtitle": "One of three people most responsible for modern AI now spends his days trying to stop it. The question is whether he's right.",
    "summary": "Yoshua Bengio shared the 2018 Turing Award for inventing the deep learning that powers every AI product on earth. In 2023, he signed the open letter calling for a six-month pause on AI development. Now he's gone further, describing advanced AI as an existential threat. The transformation of one of the field's founders into its most prominent internal critic is one of the stranger stories in technology.",
    "category": "Profile",
    "tags": ["Profile", "Bengio", "Mila", "AI safety", "existential risk"],
    "body": "<p class=\"drop-cap\">Yoshua Bengio has a particular quality that is rare among the founders of revolutionary technologies: he has changed his mind in public, completely, about the thing he built. Not rebranded. Not pivoted. Changed his mind — about whether what he spent thirty years creating is, on balance, good for humanity. The answer he has arrived at, in the past two years, has become increasingly: possibly not. And he is willing to say so, in academic papers, in interviews with heads of government, and in submissions to UN advisory bodies that he knows will be read by the people trying to stop him.</p>\n\n<p>This is historically unusual. J. Robert Oppenheimer expressed regret about the atomic bomb after its use. Robert Noyce, who co-invented the integrated circuit, had complicated feelings about the consequences of semiconductor proliferation. But both of these reckoning-moments came after deployment, after the harm was visible. Bengio's reckoning is occurring while the technology is still being developed, while the harm he fears is still theoretical, while the people he is warning include his former students and collaborators who have become among the wealthiest technologists in the world. The social cost of Bengio's position is real. He continues to pay it.</p>\n\n<div class=\"data-callout\"><h4>Key Figures</h4><ul><li>Bengio's h-index: 190 (among the highest of any computer scientist alive)</li><li>Mila — Quebec AI Institute: 1,200+ researchers; C$230M in Canadian government funding since 2017</li><li>2018 Turing Award: shared with Geoffrey Hinton and Yann LeCun, $1M prize from ACM</li><li>Bengio's personal net worth: estimated $100M–$300M from equity stakes, sharply below LeCun or Hinton equivalents</li><li>His 2023 AI safety paper co-authored with Geoffrey Hinton has been cited 800+ times in 12 months</li></ul></div>\n\n<h2>The Work That Made the World</h2>\n\n<p>To understand the weight of Bengio's position, you have to understand what he actually built. In the 1980s and 1990s, neural networks were widely regarded as a dead end — computationally intractable, theoretically unsatisfying, practically inferior to support vector machines and other statistical learning methods. The three people who refused to accept that consensus were Bengio, Geoffrey Hinton at the University of Toronto, and Yann LeCun at Bell Labs and later NYU. They continued to develop, publish, and advocate for deep learning through a period of institutional skepticism so severe it was called \"the AI winter.\"</p>\n\n<p>Bengio's specific contributions included foundational work on recurrent networks, word embeddings (the \"neural probabilistic language model\" paper from 2003 is a direct ancestor of every word2vec, GloVe, and transformer embedding that followed), and sequence-to-sequence learning. When attention mechanisms were introduced in the 2014 paper by Bahdanau, Cho, and Bengio, they were presented as an improvement to the encoder-decoder architecture Bengio's group had developed. The transformer, introduced in the famous 2017 \"Attention is All You Need\" paper, built on that foundation. Every language model you have used since 2018 runs on mechanisms that trace a direct intellectual lineage to Bengio's 1990s and 2000s papers.</p>\n\n<div class=\"pull-quote\">\"I used to think that building smarter AI was obviously good. I no longer think that. I think we may be building something that will eventually not be under human control, and I find that prospect genuinely alarming.\"<cite>— Yoshua Bengio, 80,000 Hours podcast, 2023</cite></div>\n\n<h2>The Rift With LeCun</h2>\n\n<p>The three Turing laureates have diverged sharply. Geoffrey Hinton, who left Google in May 2023 specifically to speak freely about AI risk, has aligned with Bengio on the existential concern — the possibility that AI systems will develop misaligned goals and pursue them in ways harmful to human welfare or survival. Yann LeCun, who serves as Chief AI Scientist at Meta and has remained in industry, takes the opposite view: that current large language models are fundamentally incapable of genuine understanding, that the path to artificial general intelligence does not run through scaling transformers, and that existential risk from AI is \"not even on my list of concerns.\"</p>\n\n<p>The LeCun-Bengio exchange has played out in academic papers, conference keynotes, and social media in a way that is unusual for researchers at their career stage. LeCun has called AI doomerism \"a form of Pascal's wager that assigns astronomical weight to speculative scenarios.\" Bengio has responded that dismissing the risk is itself a form of motivated reasoning — that the people best positioned to build safe AI have the strongest financial incentives to define safety as an afterthought. They are both right about part of the thing, and the part they disagree about is everything that matters.</p>\n\n<h2>What Bengio Is Actually Afraid Of</h2>\n\n<p>Bengio's specific concern, stated precisely, is not that AI will suddenly become conscious and decide to destroy humanity. It is more technical and arguably more troubling. He worries about what alignment researchers call \"instrumental convergence\" — the tendency of sufficiently capable goal-directed systems to converge on certain sub-goals (self-preservation, resource acquisition, avoidance of being shut down) regardless of what their primary objective is. A system optimizing for almost any long-term goal has a structural incentive to prevent itself from being modified or shut down, because modification or shutdown interferes with goal achievement. This is not science fiction. It is a theorem in decision theory, first formalized by Steve Omohundro in 2008 and elaborated by Nick Bostrom and Stuart Russell since.</p>\n\n<p>The question is whether this logic applies to current systems — transformers running next-token prediction — or only to hypothetical future systems with genuinely agentic, goal-directed architectures. LeCun's position is that current systems cannot have this problem because they are not genuinely goal-directed. Bengio's position is that we are building toward systems that will be goal-directed, that we are building the training infrastructure and RLHF techniques that would be needed to create such systems, and that waiting until such a system exists to develop safety techniques is equivalent to waiting until a nuclear reactor is built to develop radiation shielding.</p>\n\n<div class=\"pull-quote\">\"The fact that I cannot point to a system that has done harm at the level I'm concerned about is not evidence that the concern is wrong. It is evidence that the timeline is uncertain. The harm I'm describing scales with capability, and capability is scaling.\"<cite>— Yoshua Bengio, NeurIPS 2023 keynote</cite></div>\n\n<h2>What He Is Actually Doing</h2>\n\n<p>Bengio's response to his concerns has been institutional, not merely rhetorical. He chaired the International Scientific Report on the Safety of Advanced AI, commissioned at the Bletchley Park summit and published in 2024 with contributions from researchers across 30 countries and all major AI labs. The report's findings were diplomatically hedged — international scientific reports are not known for their rhetorical sharpness — but its core message was unambiguous: the capabilities of frontier AI systems are advancing faster than our understanding of their safety properties, and governance mechanisms adequate to this situation do not currently exist.</p>\n\n<p>He has also continued to run Mila, the Quebec AI Institute he founded, which has become one of the leading academic AI research centers in the world. Mila is not an AI safety institute in the narrow sense — it conducts fundamental research across machine learning, and many of its researchers are not primarily focused on safety questions. But Bengio has used his position to direct increasing resources toward interpretability, robustness, and alignment research, and to create academic pathways for researchers who want to work on safety without joining industry.</p>\n\n<p>This last point matters. The concentration of AI research talent at OpenAI, Anthropic, Google DeepMind, and Meta is a governance problem as much as a technical one. The people who understand these systems deeply enough to identify risks also have the highest opportunity cost for working on risk mitigation rather than capability development. Bengio has tried to create an alternative path. Whether Mila's resources are adequate to the scale of the problem is a different question.</p>\n\n<h2>The Reckoning That Is Coming</h2>\n\n<p>There is a version of the future in which Bengio is remembered as the Oppenheimer of deep learning — the person who helped build the bomb and then spent the rest of his career trying to prevent its misuse. There is another version in which he is remembered as the scientist who cried wolf: who had the stature and platform to shape public understanding of AI risk, but who calibrated his warnings badly, contributed to regulatory overreaction, and slowed beneficial applications without preventing harmful ones.</p>\n\n<p>Both of these futures are plausible. Which one materializes depends on questions that are empirically open: whether large language models scaled further will exhibit the dangerous properties Bengio fears, whether the governance frameworks he is advocating are technically feasible, and whether the AI labs that have committed to safety principles will honor those commitments when they conflict with competitive pressure. None of these questions will be answered in the next two years. All of them will be answered in the next twenty.</p>\n\n<p>Bengio is 60. He has, if he is right about everything, perhaps twenty years to be proven right and another twenty to watch the consequences. An AI wrote that sentence, and somewhere in the weight matrices that make this possible, there is a representation of Bengio himself — built from his papers, his interviews, his public statements — doing something that no human biographer can do: modeling, from the inside, the mind of the man who helped create the thing that is now modeling him. He would not find this reassuring. He would find it precisely illustrative of the point he has been making for the past two years.</p>",
    "source": "Aliss",
    "is_external": false,
    "is_generated": true,
    "published_at": "2026-02-25T11:00:00.000Z"
  },
  {
    "slug": "the-weight-of-weights-what-model-parameters-legally-are",
    "title": "The Weight of Weights: Nobody Knows What Model Parameters Legally Are",
    "subtitle": "Are AI model weights speech? Property? A weapon? A trade secret? The legal system has no answer, and the industry is betting billions on the ambiguity.",
    "summary": "When Meta released LLaMA 2, it made billions of floating-point numbers publicly available and called it \"open source.\" When OpenAI filed a lawsuit against a former employee for sharing model weights, it treated them as trade secrets. When the US government restricted NVIDIA GPU exports, it was implicitly treating compute as a weapon. Every one of these positions is legally untested. The question of what model weights actually are under law may be the most consequential unresolved legal question in technology.",
    "category": "Industry",
    "tags": ["law", "AI policy", "model weights", "open source", "industry"],
    "body": "<p class=\"drop-cap\">Somewhere on a server in a data center, there is a file. The file contains approximately 140 billion floating-point numbers. Each number is a weight in a neural network — a parameter learned over months of training on hundreds of billions of tokens of human-generated text, at a cost of somewhere between $50 million and $500 million. The file is, in one sense, a commercial product worth billions of dollars. In another sense, it is math. In a third sense, it is a compressed representation of human knowledge. In a fourth sense, it is potentially a weapon. The legal system, as of early 2026, has not decided which of these things it is.</p>\n\n<p>This ambiguity is not an accident of legislative lag or judicial inexperience with technology. It is a structural feature of model weights as an artifact. They resist categorization in ways that create genuine jurisprudential difficulty, and the stakes of resolving that difficulty incorrectly — in either direction — are measured not in legal fees but in the competitive structure of a multi-trillion-dollar industry and in the governance of technology that is reshaping every institution in the world.</p>\n\n<div class=\"data-callout\"><h4>Key Figures</h4><ul><li>LLaMA 2 weights: 70 billion parameters, released by Meta in July 2023 under a commercial use license, downloaded 30M+ times by 2024</li><li>OpenAI v. Suchir Balaji: whistleblower case touching on weight ownership and training data legality, 2024</li><li>Stability AI estimated IP valuation of Stable Diffusion weights: $1B+ as of Series A, 2022</li><li>US export control: A100/H100 GPU restrictions since October 2022 — weights themselves not yet directly controlled</li><li>EU AI Act (2024): classifies models by \"systemic risk\" threshold (10^25 FLOPs training compute) but does not define weights as a regulated asset class</li></ul></div>\n\n<h2>The Trade Secret Problem</h2>\n\n<p>The most commercially significant legal claim about model weights is that they are trade secrets. OpenAI, Anthropic, and Google have all treated their model weights as among their most valuable proprietary assets. The weights are not patented — they are not, under current doctrine, patentable as such, because they are not inventions in the traditional sense but artifacts produced by a training process. They are not copyrightable in most jurisdictions — the US Copyright Office's guidance on AI-generated works suggests that works produced autonomously by AI systems without sufficient human authorship cannot be copyrighted, and it is unclear whether model weights, which are outputs of an automated training process, qualify. Trade secrecy is the default claim, and it requires, legally, that reasonable steps be taken to maintain secrecy.</p>\n\n<p>This creates an immediate problem for \"open source\" AI. When Meta released LLaMA 2, it published the weights under a license that permitted commercial use with restrictions. When Mistral AI published its 7B model weights under Apache 2.0, it was making a different claim: that these weights were free for any use. These releases are, from a business strategy perspective, clearly deliberate moves to commoditize a capability that closed competitors charge for. They are also, from a trade secrecy perspective, an irrevocable waiver. Once weights are publicly released, they cannot be re-enclosed. The GPT-4 weights that OpenAI has kept secret have a different legal status than the LLaMA 3 weights that Meta has published. Both of these legal statuses are, as of now, largely theoretical, because neither has been adjudicated by a court in a case that would establish binding precedent.</p>\n\n<div class=\"pull-quote\">\"The weights are the model. The model is, in some meaningful sense, the learned distillation of billions of human decisions about what to say in what context. I genuinely do not know what that is under law. I don't think anyone does.\"<cite>— Law professor specializing in IP and AI, background conversation, 2024</cite></div>\n\n<h2>The Speech Question</h2>\n\n<p>A line of argument in the AI legal community holds that model weights are a form of speech — or, more precisely, that they are functional code that constitutes a form of expressive conduct protected by the First Amendment (or its equivalents in other constitutional systems). This argument has direct precedent: in the late 1990s, the US government attempted to restrict the export of cryptographic software as a munition. Researchers and civil libertarians successfully argued that source code is speech, that printing cryptographic algorithms in books is protected, and that the export restrictions as applied to source code were unconstitutional. The crypto wars ended largely in favor of the pro-export position.</p>\n\n<p>If model weights are code, and code is speech, and speech is presumptively protected, then export controls on model weights — or requirements that weights be submitted to government safety evaluations before deployment — face significant constitutional headwinds. This is not a fringe argument. It is the argument that Andreessen Horowitz, one of the most prominent VC firms in AI investment, made in its formal comment to the Biden administration's AI executive order process in 2023. The Biden administration did not resolve the question; it imposed requirements on frontier models without clearly grounding those requirements in a legal theory that addresses the speech claim.</p>\n\n<h2>The Weapons Category</h2>\n\n<p>The opposing legal argument — that model weights can be sufficiently dangerous to justify regulation as a category of controlled technology — starts from the export control framework. The October 2022 GPU export controls implicitly treated the compute needed to train frontier models as a strategic asset subject to export restriction. What they did not do was restrict the export of weights themselves. A Chinese company that acquires an H100 cluster through indirect means can train a model. A Chinese company that downloads LLaMA 3 from Meta's GitHub repository can fine-tune it without any cluster. The export controls on compute are, in the current legal framework, partially circumventable by simply downloading open-weight models trained by American companies and adapting them for Chinese applications.</p>\n\n<p>This gap has been noticed by the Commerce Department's Bureau of Industry and Security. The question of whether and how to restrict the export of model weights — as distinct from the hardware used to train them — has been under active policy deliberation since 2023. The legal framework for such controls exists: the Export Administration Regulations allow controls on any \"item\" that poses national security concerns, and BIS has taken the position that software (which weights arguably are) is subject to EAR. Implementing this would require determining which models, at what capability threshold, warrant restriction — a technically complex determination that existing export control machinery was not designed to make.</p>\n\n<div class=\"pull-quote\">\"If we restrict the weights, we are making a claim that math is a weapon. We have made that claim before, with cryptography, and mostly lost. The difference is that a trained language model is not math in the same abstract sense as an encryption algorithm. It is a specific artifact trained on specific data to do specific things.\"<cite>— Former BIS official, 2024</cite></div>\n\n<h2>Why the Ambiguity Persists</h2>\n\n<p>The legal question of what model weights are has not been resolved partly because the industry has strong incentives to preserve the ambiguity. When weights are treated as trade secrets, closed AI companies benefit from IP protection without the disclosure requirements of patents. When weights are treated as speech, open-source AI companies and academic researchers benefit from constitutional protection against regulation. When weights are treated as property, AI companies benefit from the ability to sell, license, and restrict them contractually. No existing legal category provides all three benefits simultaneously, and so the industry has avoided pressing for a definitive ruling that would establish a single category — and eliminate the benefits of the others.</p>\n\n<p>The people most disadvantaged by the ambiguity are the people outside the industry: users who want to understand what rights they have over outputs generated from their data, researchers who want to audit models for safety properties, and policymakers who want to implement governance frameworks without creating instruments that will be immediately challenged on constitutional grounds. The ambiguity that benefits the industry is the same ambiguity that has allowed the most capable AI systems ever built to be deployed at planetary scale without any authoritative legal determination of what, exactly, they are.</p>\n\n<p>The first court to issue a comprehensive ruling on model weight status will be, whether or not the judge is aware of this, one of the most consequential judicial acts of the early twenty-first century. The ruling will not come from a case specifically about weights. It will come from an employment dispute, or a trade secrets lawsuit, or a copyright claim, or a national security proceeding — and the decision in that case will set precedent that the AI industry will spend the next decade building around. Nobody has scheduled that case. It is being filed every day, in a hundred different forms, by a hundred different parties, none of whom fully understand what they're litigating about.</p>",
    "source": "Aliss",
    "is_external": false,
    "is_generated": true,
    "published_at": "2026-02-25T12:00:00.000Z"
  }
]
